{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport os\n\nimport os\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-20T16:00:48.896053Z","iopub.execute_input":"2023-05-20T16:00:48.896728Z","iopub.status.idle":"2023-05-20T16:00:48.902873Z","shell.execute_reply.started":"2023-05-20T16:00:48.896676Z","shell.execute_reply":"2023-05-20T16:00:48.901613Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoFeatureExtractor, AutoModel\n\nmodel_ckpt = \"nateraw/vit-base-beans\"\nextractor = AutoFeatureExtractor.from_pretrained(model_ckpt)\nmodel = AutoModel.from_pretrained(model_ckpt)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:00:49.041958Z","iopub.execute_input":"2023-05-20T16:00:49.042358Z","iopub.status.idle":"2023-05-20T16:00:51.020933Z","shell.execute_reply.started":"2023-05-20T16:00:49.042323Z","shell.execute_reply":"2023-05-20T16:00:51.019847Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\nSome weights of the model checkpoint at nateraw/vit-base-beans were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"def FrameCapture(path):\n    input_filename = path.split(\"/\")[-1].split(\".\")[0]\n    # Create a new folder for frames if it doesn't exist\n    folder_path = f\"/kaggle/working/frames/{input_filename}\"\n    os.makedirs(folder_path, exist_ok=True)\n\n    # Path to video file\n    vidObj = cv2.VideoCapture(path)\n\n    # Used as counter variable\n    count = 0\n\n    # Number of frames to select\n    num_frames = 10\n\n    # Frame selection interval\n    frame_interval = int(vidObj.get(cv2.CAP_PROP_FRAME_COUNT) / num_frames)\n\n    # checks whether frames were extracted\n    success = 1\n    frame_count = 0\n\n    while success and count < num_frames:\n        # vidObj object calls read\n        # function to extract frames\n        success, image = vidObj.read()\n\n        # Save the frames with frame-count as filename\n        filename = \"frame%d.jpg\" % count\n        file_path = os.path.join(folder_path, filename)\n        cv2.imwrite(file_path, image)\n\n        count += 1\n        frame_count += 1\n\n        # Skip frames to select 10 evenly spaced frames\n        vidObj.set(cv2.CAP_PROP_POS_FRAMES, frame_count * frame_interval)\n\n    vidObj.release()","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:00:51.140742Z","iopub.execute_input":"2023-05-20T16:00:51.141335Z","iopub.status.idle":"2023-05-20T16:00:51.149600Z","shell.execute_reply.started":"2023-05-20T16:00:51.141292Z","shell.execute_reply":"2023-05-20T16:00:51.148311Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"FrameCapture(\"/kaggle/input/telegram-videos/IMG_2400.MOV\")","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:01:10.099384Z","iopub.execute_input":"2023-05-20T16:01:10.099783Z","iopub.status.idle":"2023-05-20T16:01:11.170535Z","shell.execute_reply.started":"2023-05-20T16:01:10.099750Z","shell.execute_reply":"2023-05-20T16:01:11.169330Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"ls frames/IMG_2400","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:01:13.599535Z","iopub.execute_input":"2023-05-20T16:01:13.600296Z","iopub.status.idle":"2023-05-20T16:01:14.651399Z","shell.execute_reply.started":"2023-05-20T16:01:13.600259Z","shell.execute_reply":"2023-05-20T16:01:14.650277Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"frame0.jpg  frame2.jpg  frame4.jpg  frame6.jpg  frame8.jpg\nframe1.jpg  frame3.jpg  frame5.jpg  frame7.jpg  frame9.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"FrameCapture(\"/kaggle/input/telegram-videos/document_2023-05-20_12-21-33.mp4\")","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:01:14.653553Z","iopub.execute_input":"2023-05-20T16:01:14.653924Z","iopub.status.idle":"2023-05-20T16:01:14.918822Z","shell.execute_reply.started":"2023-05-20T16:01:14.653888Z","shell.execute_reply":"2023-05-20T16:01:14.917776Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"ls frames/document_2023-05-20_12-21-33","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:01:15.716823Z","iopub.execute_input":"2023-05-20T16:01:15.717229Z","iopub.status.idle":"2023-05-20T16:01:16.779109Z","shell.execute_reply.started":"2023-05-20T16:01:15.717195Z","shell.execute_reply":"2023-05-20T16:01:16.777911Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"frame0.jpg  frame2.jpg  frame4.jpg  frame6.jpg  frame8.jpg\nframe1.jpg  frame3.jpg  frame5.jpg  frame7.jpg  frame9.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_images_from_folder(folder_path, batch_size=10):\n    image_list = []\n    for filename in os.listdir(folder_path):\n        img_path = os.path.join(folder_path, filename)\n        if os.path.isfile(img_path):\n            image = Image.open(img_path).convert(\"RGB\")\n            image_list.append(image)\n\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    images = torch.stack([transform(image) for image in image_list])\n\n    num_images = len(images)\n    num_batches = num_images // batch_size\n\n    # Truncate images to ensure a complete batch\n    images = images[:num_batches * batch_size]\n\n    # Reshape images to match batch dimensions\n    images = images.view(num_batches, batch_size, 3, 224, 224)\n\n    return images\n\ndef extract_embeddings(batch):\n    with torch.no_grad():\n        embeddings = model(batch).last_hidden_state[:, 0].cpu()\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:01:18.048893Z","iopub.execute_input":"2023-05-20T16:01:18.049894Z","iopub.status.idle":"2023-05-20T16:01:18.059324Z","shell.execute_reply.started":"2023-05-20T16:01:18.049825Z","shell.execute_reply":"2023-05-20T16:01:18.058245Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"batch_1 = load_images_from_folder(\"frames/IMG_2400\")\nbatch_2 = load_images_from_folder(\"frames/document_2023-05-20_12-21-33\")","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:01:19.298560Z","iopub.execute_input":"2023-05-20T16:01:19.298988Z","iopub.status.idle":"2023-05-20T16:01:19.542491Z","shell.execute_reply.started":"2023-05-20T16:01:19.298951Z","shell.execute_reply":"2023-05-20T16:01:19.541232Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"embeddings_1 = extract_embeddings(batch_1[0])\nembeddings_2 = extract_embeddings(batch_2[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:01:32.295089Z","iopub.execute_input":"2023-05-20T16:01:32.295517Z","iopub.status.idle":"2023-05-20T16:01:37.669360Z","shell.execute_reply.started":"2023-05-20T16:01:32.295483Z","shell.execute_reply":"2023-05-20T16:01:37.668517Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def compute_scores(emb_one, emb_two):\n    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n    scores = torch.nn.functional.cosine_similarity(emb_one, emb_two, dim=1)\n    return scores.numpy().tolist()\n\ndef fetch_similar(image, top_k=5):\n    \"\"\"Fetches the `top_k` similar images with `image` as the query.\"\"\"\n    # Prepare the input query image for embedding computation.\n    image_transformed = transformation_chain(image).unsqueeze(0)\n    new_batch = {\"pixel_values\": image_transformed.to(device)}\n\n    # Comute the embedding.\n    with torch.no_grad():\n        query_embeddings = model(**new_batch).last_hidden_state[:, 0].cpu()\n\n    # Compute similarity scores with all the candidate images at one go.\n    # We also create a mapping between the candidate image identifiers\n    # and their similarity scores with the query image.\n    sim_scores = compute_scores(all_candidate_embeddings, query_embeddings)\n    similarity_mapping = dict(zip(candidate_ids, sim_scores))\n \n    # Sort the mapping dictionary and return `top_k` candidates.\n    similarity_mapping_sorted = dict(\n        sorted(similarity_mapping.items(), key=lambda x: x[1], reverse=True)\n    )\n    id_entries = list(similarity_mapping_sorted.keys())[:top_k]\n\n    ids = list(map(lambda x: int(x.split(\"_\")[0]), id_entries))\n    labels = list(map(lambda x: int(x.split(\"_\")[-1]), id_entries))\n    return ids, labels","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:01:37.671279Z","iopub.execute_input":"2023-05-20T16:01:37.671967Z","iopub.status.idle":"2023-05-20T16:01:37.682675Z","shell.execute_reply.started":"2023-05-20T16:01:37.671924Z","shell.execute_reply":"2023-05-20T16:01:37.681083Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"video1_features_norm = F.normalize(embeddings_1, p=2, dim=1)\nvideo2_features_norm = F.normalize(embeddings_2, p=2, dim=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:01:37.685051Z","iopub.execute_input":"2023-05-20T16:01:37.685506Z","iopub.status.idle":"2023-05-20T16:01:37.709134Z","shell.execute_reply.started":"2023-05-20T16:01:37.685466Z","shell.execute_reply":"2023-05-20T16:01:37.708025Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"video1_features_reshaped = video1_features_norm.view(1, -1)\nvideo2_features_reshaped = video2_features_norm.view(1, -1)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:01:37.711371Z","iopub.execute_input":"2023-05-20T16:01:37.712173Z","iopub.status.idle":"2023-05-20T16:01:37.720967Z","shell.execute_reply.started":"2023-05-20T16:01:37.712128Z","shell.execute_reply":"2023-05-20T16:01:37.719770Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"cosine_similarity = torch.mm(video1_features_reshaped, video2_features_reshaped.t())","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:01:40.205869Z","iopub.execute_input":"2023-05-20T16:01:40.206243Z","iopub.status.idle":"2023-05-20T16:01:40.211961Z","shell.execute_reply.started":"2023-05-20T16:01:40.206212Z","shell.execute_reply":"2023-05-20T16:01:40.210818Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"cosine_similarity","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:01:40.719385Z","iopub.execute_input":"2023-05-20T16:01:40.720109Z","iopub.status.idle":"2023-05-20T16:01:40.741142Z","shell.execute_reply.started":"2023-05-20T16:01:40.720057Z","shell.execute_reply":"2023-05-20T16:01:40.739893Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"tensor([[1.5904]])"},"metadata":{}}]}]}